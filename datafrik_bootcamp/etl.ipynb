{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DB CREDENTIALS\n",
    "load_dotenv()\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT= os.getenv(\"DB_PORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONNECTION_STRING = f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_path = \"/Users/primrose/Desktop/assignment/.venv/data/employees.csv\"\n",
    "customers_path = \"/Users/primrose/Desktop/assignment/.venv/data/customers.csv\"\n",
    "products_path = \"/Users/primrose/Desktop/assignment/.venv/data/products.csv\"\n",
    "stores_path = \"/Users/primrose/Desktop/assignment/.venv/data/stores.csv\"\n",
    "orders_path = \"/Users/primrose/Desktop/assignment/.venv/data/orders.json\"\n",
    "\n",
    "def extract():\n",
    "    \"\"\"\"\n",
    "    Extract data from the files and return a tuple of dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    df_employees = pd.read_csv(employees_path) if os.path.exists(employees_path) else pd.DataFrame()\n",
    "    df_customers = pd.read_csv(customers_path) if os.path.exists(customers_path) else pd.DataFrame()\n",
    "    df_products = pd.read_csv(products_path) if os.path.exists(products_path) else pd.DataFrame()\n",
    "    df_stores = pd.read_csv(stores_path) if os.path.exists(stores_path) else pd.DataFrame()\n",
    "\n",
    "    with open(orders_path, \"r\") as file:\n",
    "        orders = json.load(file)\n",
    "\n",
    "    return df_employees.to_dict(), df_customers.to_dict(), df_products.to_dict(),df_stores.to_dict(),orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"   \n",
    "    Transform the data inorder to match the expected data format in our destination db i.e\n",
    "    - Full_Name in customer + employee table\n",
    "    - Order_items table\n",
    "    - Ensure validity of data being entered\n",
    "    - No duplicate values\n",
    "    - Fill the missing data e.g Employee data has missing phone number and emails.\n",
    "    \"\"\"  \n",
    "\n",
    "   # Check validity\n",
    "\n",
    "def is_valid_emaiil(email):\n",
    "   \n",
    "   #Returns True is email follows a valid format, else False\n",
    "\n",
    "  pattern = r\"ˆ[a-zA-Z0-9_.+]+@[a-zA-Z0-9]+\\.[a-zA-Z0-9-.]+$\"\n",
    "  return bool(re.match(pattern,email))\n",
    "\n",
    "\n",
    "def is_valid_phone(phone):\n",
    "  pattern = r\"ˆ\\+?\\d{1,10}$\"\n",
    "  return bool(re.match(pattern,phone))\n",
    "\n",
    "extracted_data = extract()\n",
    "\n",
    "\n",
    "def transform(extracted_data):\n",
    "\n",
    "  # We transform dataframes hence, change the dicts back to dataframes\n",
    "  df_employee = pd.DataFrame(extracted_data[0])\n",
    "  df_customers = pd.DataFrame(extracted_data[1])\n",
    "  df_products = pd.DataFrame(extracted_data[2])\n",
    "  df_stores = pd.DataFrame(extracted_data[3])\n",
    "\n",
    "  \n",
    "  #print(df_employee)\n",
    "\n",
    "  # Fill missing values\n",
    "  for col in [\"Phone\",\"Email\",\"First_Name\",\"Last_Name\"]:\n",
    "    if col not in df_employee.columns:\n",
    "      df_employee[col]  = np.nan\n",
    "\n",
    "  for col in [\"Phone\",\"Email\",\"First_Name\",\"Last_Name\"]:\n",
    "    if col not in df_customers.columns:\n",
    "      df_customers[col] = np.nan\n",
    "\n",
    "  \n",
    "  #print(df_products)\n",
    "  # drop records that lack crictical data i.e Primary key columns in db\n",
    "\n",
    "  df_customers = df_customers.dropna(subset = [\"Customer_ID\"])\n",
    "  df_employee = df_employee.dropna(subset = [\"Employee_ID\"])\n",
    "  df_stores = df_stores.dropna(subset = [\"Store_ID\"])\n",
    "  df_products = df_products.dropna(subset=[\"Product_ID\"])\n",
    "  \n",
    "  # Check validity and replace invalid fields\n",
    "  df_employee[\"Email\"] = df_employee[\"Email\"].apply(lambda x: x if is_valid_emaiil(str(x)) else \"Unknown\")\n",
    "  df_customers[\"Email\"] = df_customers[\"Email\"].apply(lambda x: x if is_valid_emaiil(str(x)) else \"Unknown\")\n",
    "\n",
    "  df_employee[\"Phone\"] = df_employee[\"Phone\"].apply(lambda x: x if is_valid_phone(str(x)) else \"Unknown\")\n",
    "  df_customers[\"Phone\"] = df_customers[\"Phone\"].apply(lambda x: x if is_valid_phone(str(x)) else \"Unknown\")\n",
    "\n",
    "  # Merge our Full_Name bt first fill that record if value is missing\n",
    "\n",
    "  df_customers[\"Full_Name\"] = df_customers[\"First_Name\"].fillna(\"\") + \" \" + df_customers[\"Last_Name\"].fillna(\"\")\n",
    "  df_employee[\"Full_Name\"] = df_employee[\"First_Name\"].fillna(\"\") + \" \" + df_employee[\"Last_Name\"].fillna(\"\")\n",
    "\n",
    "  # drop First name and last name columns since we have the full name - no redundancy\n",
    "  df_employee.drop(columns=[\"First_Name\",\"Last_Name\"], inplace=True)\n",
    "  df_customers.drop(columns=[\"First_Name\",\"Last_Name\"], inplace=True)\n",
    "\n",
    "  # Drop duplicates\n",
    "  df_employee.drop_duplicates(inplace=True)\n",
    "  df_customers.drop_duplicates(inplace=True)\n",
    "  df_stores.drop_duplicates(inplace=True)\n",
    "  df_products.drop_duplicates(inplace=True)\n",
    "\n",
    "  # Get orders and order_items from the orders json data\n",
    "  orders = []\n",
    "  order_items = []\n",
    "  orders_json = extracted_data[4]\n",
    "\n",
    "  for order in orders_json:\n",
    "    orders.append([\n",
    "      order[\"Order_ID\"],order[\"Customer_ID\"],order[\"Order_Date\"],order[\"Store_ID\"],order[\"Total_Amount\"]])\n",
    "    for item in order['Items']:\n",
    "      order_items.append([order['Order_ID'],item['Product_ID'],item['Quantity'],item['Unit_Price']])\n",
    "    \n",
    "  # Transform the lists into a dataframe so we can do further transformations and later dump into db\n",
    "\n",
    "  df_orders = pd.DataFrame(orders,columns=[\"Order_ID\",\"Customer_ID\",\"Order_Date\",\"Store_ID\",\"Total_Amount\"])\n",
    "\n",
    "  df_order_items = pd.DataFrame(order_items, columns=[\"Order_ID\",\"Product_ID\",\"Quantity\",\"Unit_Price\"])\n",
    "  df_order_items.insert(0, 'Order_Item_ID', range(1, len(df_order_items) + 1))\n",
    "\n",
    "  # Per usual, drop duplicates\n",
    "\n",
    "  df_orders.drop_duplicates(inplace=True)\n",
    "  df_order_items.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "  return df_employee.to_dict(),df_customers.to_dict(),df_products.to_dict(),df_stores.to_dict(),df_orders.to_dict(),df_order_items.to_dict()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_Data = transform(extracted_data)\n",
    "def load(transformed_Data):\n",
    "  df_employee = pd.DataFrame(transformed_Data[0])\n",
    "  df_customers = pd.DataFrame(transformed_Data[1])\n",
    "  df_products = pd.DataFrame(transformed_Data[2])\n",
    "  df_stores = pd.DataFrame(transformed_Data[3])\n",
    "  df_orders = pd.DataFrame(transformed_Data[4])\n",
    "  df_order_items = pd.DataFrame(transformed_Data[5])\n",
    "\n",
    "  print(df_stores)\n",
    "\n",
    "  engine = create_engine(DB_CONNECTION_STRING)\n",
    "\n",
    "  if not df_employee.empty:\n",
    "    df_employee.to_sql(\"employees\", engine, if_exists=\"append\", index=False)\n",
    "  \n",
    "  if not df_customers.empty:\n",
    "    df_customers.to_sql(\"customers\", engine, if_exists=\"append\",index=False)\n",
    "\n",
    "  if not df_products.empty:\n",
    "    df_products.to_sql(\"products\", engine, if_exists=\"append\",index=False)\n",
    "\n",
    "  if not df_stores.empty:\n",
    "    df_stores.to_sql(\"stores\", engine, if_exists=\"append\",index=False)\n",
    "\n",
    "  if not df_orders.empty:\n",
    "    df_orders.to_sql(\"orders\", engine, if_exists=\"append\",index=False)\n",
    "\n",
    "  if not df_order_items.empty:\n",
    "    df_order_items.to_sql(\"order_items\", engine, if_exists=\"append\",index=False)\n",
    "\n",
    "load(transformed_Data)\n",
    "print(\"Data loaded successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
